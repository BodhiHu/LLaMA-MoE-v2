{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aea6bb51fed4877a89033a532410a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ce6407c34047bbab17ca90279ec13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4c88cbcc5c460195862112cd7efdff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4f252d9b7f493aad0ee24f8edbdd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/138384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11be08ddd93478593bb57d68aa9af77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/17944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734f05e85b7e4357a89cfe112eabe4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/17210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trivia_qa\",'rc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "def main():\n",
    "    # 加载tokenizer和模型\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"/home/data/models/llama-transformers/7B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = LlamaForSequenceClassification.from_pretrained(\"/home/data/models/llama-transformers/7B\")\n",
    "\n",
    "    # 准备训练参数，并设置 GPU 的数量\n",
    "    args = TrainingArguments(\n",
    "        \"test-triviaqa\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        per_device_eval_batch_size=4,\n",
    "    )\n",
    "\n",
    "    # 这里选择了 'rc' 配置，但您可以根据需要选择其他配置\n",
    "    dataset = load_dataset(\"trivia_qa\", \"rc\")\n",
    "    \n",
    "    # 请注意，trivia_qa 可能不在 GLUE 指标中。您可能需要选择或实现一个合适的评估指标。\n",
    "    metric = load_metric('trivia_qa', 'rc')  \n",
    "\n",
    "    def encode(examples):\n",
    "        # 更新以匹配 trivia_qa 的数据结构\n",
    "        return tokenizer(examples['question'], examples['answer'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "    encoded_dataset = dataset.map(encode, batched=True)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        eval_dataset=encoded_dataset[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    print(f\"Results for trivia_qa: {eval_results}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 加载tokenizer和模型\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"/home/data/models/llama-transformers/7B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = LlamaForSequenceClassification.from_pretrained(\"/home/data/models/llama-transformers/7B\")\n",
    "\n",
    "    # 准备训练参数，并设置 GPU 的数量\n",
    "    args = TrainingArguments(\n",
    "        \"test-glue\",\n",
    "        evaluation_strategy=\"steps\",  # 设置为“steps”以在指定的步数后进行评估\n",
    "        eval_steps=500,  # 每500步进行一次评估\n",
    "        per_device_eval_batch_size=4,  # 每个设备上的评估批次大小\n",
    "    )\n",
    "\n",
    "    # 定义 GLUE 任务列表\n",
    "    tasks = ['cola', 'sst2', 'mrpc', 'stsb', 'qqp', 'mnli', 'qnli', 'rte', 'wnli']\n",
    "\n",
    "    for task in tasks:\n",
    "        # 加载数据集和指标\n",
    "        dataset = load_dataset(\"glue\", task)\n",
    "        metric = load_metric('glue', task)\n",
    "\n",
    "        # 根据任务类型调整编码函数\n",
    "        def encode(examples):\n",
    "            # 对于双句子任务\n",
    "            if task in ['mrpc', 'stsb', 'qqp', 'mnli', 'qnli', 'rte']:\n",
    "                return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=256)\n",
    "            # 对于单句子任务\n",
    "            else:\n",
    "                return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "        # 对数据集进行预处理\n",
    "        encoded_dataset = dataset.map(encode, batched=True)\n",
    "\n",
    "        # 定义计算评估指标的函数\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = predictions.argmax(axis=-1)\n",
    "            return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "        # 创建Trainer\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            args,\n",
    "            eval_dataset=encoded_dataset[\"validation\"],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        # 进行评估并获取结果\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        # 打印结果\n",
    "        print(f\"Results for {task}: {eval_results}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
